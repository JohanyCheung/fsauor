{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from config import Config\n",
    "# from nlp.utils.pinyin import PinYin\n",
    "from pypinyin import lazy_pinyin, pinyin\n",
    "\n",
    "myconf = Config()\n",
    "random.seed = myconf.random.seed\n",
    "# pinyin = PinYin(\"./dict/word.data\")\n",
    "# pinyin.load_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    with open(\"dict/stopwords.txt\") as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "punctuation_zh = \".ã€ï¼Œã€‚Â°ï¼Ÿï¼ï¼šï¼›â€œâ€â€™â€˜ï½žâ€¦ã€ã€‘ã€Žã€ï¼»ï¼½ï¼ˆï¼‰()ã€Šã€‹ï½›ï½Ã—â€•ï¼Â·â€¢â†’â„ƒã€ˆã€‰\"\n",
    "spec = ['\"', ' ', '\\xa0', '\\n', '\\ufeff', '\\r', '\\t']\n",
    "# spec = ['\\xa0', '\\n', '\\ufeff', '\\r', '\\t']\n",
    "# Combination of Chinese and English punctuation.\n",
    "punctuation_all = punctuation + punctuation_zh\n",
    "# è¡¨æƒ…\n",
    "emoticon = myconf.data.emoticon\n",
    "# å¥å°¾è¯­æ°”è¯\n",
    "tone_words = \"çš„äº†å‘¢å§å—å•Šå•¦å‘€ä¹ˆå˜›å“’å“¼\"\n",
    "\n",
    "# char level\n",
    "# 1.ä¸åŽ»\n",
    "filter_characters = spec\n",
    "# 2.åŽ»æŽ‰ æ ‡ç‚¹+åœç”¨è¯+è¡¨æƒ…ç¬¦å·\n",
    "# filter_characters = list(set(punctuation_all) | set(get_stopwords()) | set(emoticon)) + spec\n",
    "# 3.åŽ»æŽ‰ æ ‡ç‚¹+è¡¨æƒ…ç¬¦å·\n",
    "# filter_characters = list(set(punctuation_all) | set(emoticon)) + spec\n",
    "# 4.åŽ»æŽ‰ æ ‡ç‚¹\n",
    "# filter_characters = list(set(punctuation_all)) + spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_filter(text):\n",
    "    \"\"\"è¿‡æ»¤åˆ†è¯ã€‚\n",
    "    \"\"\"\n",
    "    # å¥å°¾æ ‡ç‚¹ç¬¦å·è¿‡æ»¤\n",
    "    s = text.rstrip(punctuation_all)\n",
    "    # å¥å°¾è¯­æ°”è¯è¿‡æ»¤\n",
    "    s = s.rstrip(tone_words)\n",
    "    # å¥ä¸­ç»¼åˆè¿‡æ»¤\n",
    "    words = [w for w in jieba.cut(s) if w not in filter_characters]\n",
    "    return words\n",
    "\n",
    "def seg_base(text):\n",
    "    words = jieba.lcut(text, cut_all=False)\n",
    "    return words\n",
    "  \n",
    "def filter_char(text, sep=' '):\n",
    "    \"\"\"Remove filter_characters and generate char sequence\n",
    "    \"\"\"\n",
    "    res = [c for c in text if c not in filter_characters]\n",
    "    return sep.join(res)\n",
    "\n",
    "def get_chars(text):\n",
    "    res = [c for c in text if c not in filter_characters]\n",
    "    return res\n",
    "\n",
    "def get_pinyin(text):\n",
    "    res = [pinyin.char2pinyin(c) for c in text if c not in filter_characters]\n",
    "    return res\n",
    "\n",
    "def get_pinyin2(text, tone=False):\n",
    "    res = filter_char(text, sep='')\n",
    "    if tone:\n",
    "        return lazy_pinyin(res, 1)\n",
    "    else:\n",
    "        return lazy_pinyin(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    å¼ å¼ å¼ ï¼Œ èŒ æ­» äºº çš„ æ£’ æ£’ ç³– ï¼Œ ä¸­ äº† å¤§ ä¼— ç‚¹ è¯„ çš„ éœ¸ çŽ‹ é¤ ï¼Œ ...\n",
       "1    ç¬¬ ä¸‰ æ¬¡ å‚ åŠ  å¤§ ä¼— ç‚¹ è¯„ ç½‘ éœ¸ çŽ‹ é¤ çš„ æ´» åŠ¨ ã€‚ è¿™ å®¶ åº— ç»™ äºº æ•´ ...\n",
       "2    4 äºº åŒ è¡Œ ç‚¹ äº† 1 0 ä¸ª å° åƒ æ¦´ èŽ² é…¥ æ¦´ èŽ² å‘³ é“ ä¸ è¶³ æ¾ è½¯ å¥¶ ...\n",
       "3    ä¹‹ å‰ è¯„ ä»· äº† èŽ« å å…¶ å¦™ è¢« åˆ  æžœ æ–­ ç»§ ç»­ å·® è¯„ ï¼ æ¢ äº† èœ å• ä»· ...\n",
       "4    å‡º ä¹Ž æ„ æ–™ åœ° æƒŠ è‰³ ï¼Œ æ¤° å­ é¸¡ æ¸… çƒ­ é™ ç« ï¼Œ ç¾Ž å®¹ å…» é¢œ ï¼Œ å¤§ å¤§ ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(myconf.data.train_download)\n",
    "train.content = train.content.map(lambda x: filter_char(x))\n",
    "# train.content = train.content.map(lambda x: get_chars(x))\n",
    "# train.content = train.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "train[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(myconf.data.train, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    å“Ž ï¼Œ æƒ³ å½“ å¹´ æ¥ ä½˜ å±± çš„ æ—¶ å€™ ï¼Œ å•¥ éƒ½ æ²¡ æœ‰ ï¼Œ ä¸‰ å“ é¦™ ç®— é•‡ ä¸Š ...\n",
       "1    è¶ ç€ å›½ åº† èŠ‚ ï¼Œ ä¸€ å®¶ äºº åœ¨ ç™½ å¤© åœ¨ å±± é‡Œ çŽ© è€ ä¹‹ åŽ ï¼Œ æ™š ä¸Š å†³ ...\n",
       "2    è¿™ å®¶ åº— æ˜¯ æˆ‘ ç›® å‰ åƒ åˆ° çš„ æœ€ å¹² å‡€ çš„ ä¸² ä¸² åº— ï¼Œ ç›® å‰ çœ‹ åˆ° æœ€ ...\n",
       "3    ä¸­ åˆ å’Œ é¢† å¯¼ å· å· è·‘ å‡º æ¥ å¼€ å° ç¶ å•¦ ï¼Œ åˆ° è¿™ å®¶ æ¥ é¡¿ æ–™ ç† ï¼Œ ...\n",
       "4    æ‹– äº† å¾ˆ ä¹… å¾ˆ ä¹… æ‰ æ¥ è¡¥ çš„ ç‚¹ è¯„ ï¼Œ ä¹‹ å‰ å’Œ æœ‹ å‹ ä¸€ å— å„¿ åŽ» çš„ ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv(myconf.data.val_download)\n",
    "val.content = val.content.map(lambda x: filter_char(x))\n",
    "# val.content = val.content.map(lambda x: get_chars(x))\n",
    "# val.content = val.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "val[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv(myconf.data.val, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    æˆ‘ æƒ³ è¯´ ä»– ä»¬ å®¶ çš„ ä¼˜ æƒ  æ´» åŠ¨ å¥½ æŒ ä¹… å•Š ï¼Œ æˆ‘ é¢„ å”® çš„ æ—¶ å€™ ä¹° ...\n",
       "1    ç»ˆ äºŽ å¼€ åˆ° å¿ƒ å¿ƒ å¿µ å¿µ çš„ L A B l o f t ã€‚ ç¬¬ ä¸€ æ¬¡ æ¥ å°± éš ...\n",
       "2    åœ° ç† ä½ ç½® å¥½ ï¼Œ äº¤ é€š æ–¹ ä¾¿ ï¼Œ å°± åœ¨ 1 2 4 è½¦ ç«™ å¯¹ é¢ äº¤ é€š æ–¹ ...\n",
       "3    è¿ æ°” å¾ˆ å¥½ ï¼Œ æŠ½ ä¸­ äº† å¤§ ä¼— ç‚¹ è¯„ çš„ éœ¸ çŽ‹ é¤ ã€‚ è¿™ å®¶ ä¸» é¢˜ é¤ åŽ… ...\n",
       "4    å¹¸ è¿ éš ç‚¹ è¯„ å›¢ ä½“ éªŒ éœ¸ çŽ‹ é¤ ï¼Œ å¿ƒ æƒ… å¥½ ~ èœ€ ä¹ é¦™ åˆš è¿› é©» æ³‰ ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testa = pd.read_csv(myconf.data.testa_download)\n",
    "testa.content = testa.content.map(lambda x: filter_char(x))\n",
    "# testa.content = testa.content.map(lambda x: get_chars(x))\n",
    "# testa.content = testa.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "testa[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testa.to_csv(myconf.data.testa, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    å¯ ä»¥ è¯´ å·¥ ä½œ æ—¥ ä¸­ åˆ çš„ è¿™ ä¸ª å¥— é¤ ç€ å®ž æ˜¯ ç‰¹ åˆ« çš„ å®ž æƒ  å•Š ï¼Œ ...\n",
       "1    ä½ ç½® å¾ˆ ä¸ é”™ ï¼Œ è€ æ¿ çœ¼ å…‰ å¾ˆ å¥½ ðŸ‘ ä½ äºŽ åœ° é“ ç«™ é™„ è¿‘ ï¼Œ å‘¨ å›´ ...\n",
       "2    å’Œ æœ‹ å‹ å° èš ï¼Œ é€‰ æ‹© äº† å±± ä¸Š ä¸‹ . ã€Œ é¾™ è™¾ æ²™ æ‹‰ ã€ æœ‹ å‹ ä¸ å–œ ...\n",
       "3    åœ° å€ : æ¹– æ»¨ è·¯ åŒ— 1 0 5 8 å· ï¼ˆ å›½ ç¨Ž å±€ å— ä¾§ ï¼‰ ä¸œ æ¥ é¡º å¾€ ...\n",
       "4    å…ƒ æ—¦ ä¸­ åˆ æ¥ åƒ é¥­ ï¼Œ ä¸ƒ ä¸ª å¤§ äºº ä¸€ ä¸ª å° å­© ã€‚ ç¬¬ ä¸€ æ¬¡ åƒ ä¸œ åŒ— ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testb = pd.read_csv(myconf.data.testb_download)\n",
    "testb.content = testb.content.map(lambda x: filter_char(x))\n",
    "# testb.content = testb.content.map(lambda x: get_chars(x))\n",
    "# testb.content = testb.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "testb[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testb.to_csv(myconf.data.testb, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(srcfile=None, desfile=None, func=None, mode='w'):\n",
    "    \"\"\"æŒ‰è¡Œè¯»å–æºæ–‡ä»¶è¿›è¡Œè‡ªå®šä¹‰å¤„ç†ï¼Œå°†ç»“æžœä¿å­˜åˆ°ç›®æ ‡æ–‡ä»¶ã€‚\n",
    "    ç›´åˆ°é‡åˆ°ç©ºè¡Œæˆ–è€…åˆ°è¾¾æ–‡ä»¶æœ«å°¾ä¸ºæ­¢ã€‚\n",
    "    \"\"\"\n",
    "    assert srcfile is not None, \"srcfile can not be None\"\n",
    "    assert desfile is not None, \"desfile can not be None\"\n",
    "    assert func is not None, \"func can not be None\"\n",
    "    with open(desfile, mode, encoding='UTF-8') as des:\n",
    "        with open(srcfile, 'r', encoding='UTF-8') as src:\n",
    "            for line in iter(src.readline, ''):\n",
    "                des.write(func(line))\n",
    "    print(\"process_line complete!\")\n",
    "\n",
    "def build_seq(line):\n",
    "    \"\"\"Build seq for train data\"\"\"\n",
    "    items = line.rstrip().split(',')\n",
    "    tag = ' '.join(items[2:])\n",
    "    return items[1] + '\\t' + tag + '\\n'\n",
    "\n",
    "def build_seq_test(line):\n",
    "    \"\"\"Build seq for test data\"\"\"\n",
    "    items = line.rstrip().split(',')\n",
    "    return items[1] + '\\n'\n",
    "\n",
    "def build_text(line):\n",
    "    \"\"\"Build text for word2vec\"\"\"\n",
    "    items = line.rstrip().split('\\t')\n",
    "    return items[0] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_line complete!\n",
      "process_line complete!\n",
      "process_line complete!\n"
     ]
    }
   ],
   "source": [
    "# build data for seq2seq\n",
    "# ä½¿ç”¨ filter_char å¤„ç†åŽåŽ»é™¤è¡¨å¤´ç”Ÿæˆ myconf.data\n",
    "process_line(srcfile=myconf.data.train, desfile=\"preprocess/train.tsv\", func=build_seq)\n",
    "process_line(srcfile=myconf.data.val, desfile=\"preprocess/val.tsv\", func=build_seq)\n",
    "process_line(srcfile=myconf.data.test, desfile=\"preprocess/test.tsv\", func=build_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_line complete!\n",
      "process_line complete!\n",
      "process_line complete!\n"
     ]
    }
   ],
   "source": [
    "# build data for word2vec\n",
    "# tsv æ— è¡¨å¤´\n",
    "process_line(srcfile=\"preprocess/base/train.tsv\", desfile=\"preprocess/base/train.txt\", func=build_text, mode='w')\n",
    "process_line(srcfile=\"preprocess/base/val.tsv\", desfile=\"preprocess/base/dev.txt\", func=build_text, mode='w')\n",
    "process_line(srcfile=\"preprocess/base/test.tsv\", desfile=\"preprocess/base/test.txt\", func=build_text, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_lines(filepath, start=0, nmax=0):\n",
    "    \"\"\"æŒ‰è¡Œè¯»å–æ–‡ä»¶ï¼Œè¯»å–æŒ‡å®šè¡Œæ•° nmaxï¼Œå½“ nmax=0 æ—¶æŒ‰è¡Œè¯»å–å…¨éƒ¨\n",
    "    \"\"\"\n",
    "    fp = open(filepath, 'r', encoding='UTF-8')\n",
    "    if start > 0:\n",
    "        for i in range(start):\n",
    "            fp.readline()\n",
    "    n = 0\n",
    "    while True:\n",
    "        content = fp.readline()\n",
    "        if content == '' or (n >= nmax and nmax != 0):\n",
    "            break\n",
    "        n += 1\n",
    "        yield content      \n",
    "    fp.close()\n",
    "\n",
    "def cut_valid(filepath='./preprocessval.tsv', n=None, pos=None):\n",
    "    assert n is not None, \"n can not be None\"\n",
    "    assert pos is not None, \"pos can not be None\"\n",
    "    with open('./preprocess/val_{}.tsv'.format(str(pos)), 'w', encoding='UTF-8') as f:\n",
    "        for line in read_file_lines(filepath, start=0, nmax=pos):\n",
    "            f.write(line)\n",
    "    with open('./preprocess/val_{}.tsv'.format(str(n-pos)), 'w', encoding='UTF-8') as f:\n",
    "        for line in read_file_lines(filepath, start=pos, nmax=n-pos):\n",
    "            f.write(line)\n",
    "\n",
    "cut_valid(filepath='./preprocess/val.tsv', n=15000, pos=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# sentences = []\n",
    "# with open(\"./preprocess/chars.txt\", 'r', encoding='UTF-8') as src:\n",
    "#     for line in iter(src.readline, ''):\n",
    "#         sentences.append(line.rstrip().split())\n",
    "        \n",
    "frames = [train[\"content\"], val[\"content\"], testa[\"content\"], testb[\"content\"]]\n",
    "content = pd.concat(frames)\n",
    "with open(\"chars.txt\", 'w', encoding='UTF-8') as f:\n",
    "    for s in content:\n",
    "        f.write(s + '\\n')\n",
    "# sentences = [s for s in content]\n",
    "\n",
    "# word2vec_model = Word2Vec(\n",
    "#     sentences,\n",
    "#     size=100,\n",
    "#     window=5,\n",
    "#     min_count=1,\n",
    "#     workers=4,\n",
    "#     iter=15\n",
    "#     )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format(\"embedding/pinyin_all_wv.vector\", binary=True)\n",
    "word2vec_model.save(\"embedding/pinyin_all.vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format(\"embedding/chars_all_wv.vector\", binary=True)\n",
    "word2vec_model.save(\"embedding/chars_all.vector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
