{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from config import Config\n",
    "# from nlp.utils.pinyin import PinYin\n",
    "from pypinyin import lazy_pinyin, pinyin\n",
    "\n",
    "myconf = Config()\n",
    "random.seed = myconf.random.seed\n",
    "# pinyin = PinYin(\"./dict/word.data\")\n",
    "# pinyin.load_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    with open(\"dict/stopwords.txt\") as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "punctuation_zh = \".、，。°？！：；“”’‘～…【】『』［］（）()《》｛｝×―－·•→℃〈〉\"\n",
    "spec = ['\"', ' ', '\\xa0', '\\n', '\\ufeff', '\\r', '\\t']\n",
    "# spec = ['\\xa0', '\\n', '\\ufeff', '\\r', '\\t']\n",
    "# Combination of Chinese and English punctuation.\n",
    "punctuation_all = punctuation + punctuation_zh\n",
    "# 表情\n",
    "emoticon = myconf.data.emoticon\n",
    "# 句尾语气词\n",
    "tone_words = \"的了呢吧吗啊啦呀么嘛哒哼\"\n",
    "\n",
    "# char level\n",
    "# 1.不去\n",
    "filter_characters = spec\n",
    "# 2.去掉 标点+停用词+表情符号\n",
    "# filter_characters = list(set(punctuation_all) | set(get_stopwords()) | set(emoticon)) + spec\n",
    "# 3.去掉 标点+表情符号\n",
    "# filter_characters = list(set(punctuation_all) | set(emoticon)) + spec\n",
    "# 4.去掉 标点\n",
    "# filter_characters = list(set(punctuation_all)) + spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_filter(text):\n",
    "    \"\"\"过滤分词。\n",
    "    \"\"\"\n",
    "    # 句尾标点符号过滤\n",
    "    s = text.rstrip(punctuation_all)\n",
    "    # 句尾语气词过滤\n",
    "    s = s.rstrip(tone_words)\n",
    "    # 句中综合过滤\n",
    "    words = [w for w in jieba.cut(s) if w not in filter_characters]\n",
    "    return words\n",
    "\n",
    "def seg_base(text):\n",
    "    words = jieba.lcut(text, cut_all=False)\n",
    "    return words\n",
    "  \n",
    "def filter_char(text, sep=' '):\n",
    "    \"\"\"Remove filter_characters and generate char sequence\n",
    "    \"\"\"\n",
    "    res = [c for c in text if c not in filter_characters]\n",
    "    return sep.join(res)\n",
    "\n",
    "def get_chars(text):\n",
    "    res = [c for c in text if c not in filter_characters]\n",
    "    return res\n",
    "\n",
    "def get_pinyin(text):\n",
    "    res = [pinyin.char2pinyin(c) for c in text if c not in filter_characters]\n",
    "    return res\n",
    "\n",
    "def get_pinyin2(text, tone=False):\n",
    "    res = filter_char(text, sep='')\n",
    "    if tone:\n",
    "        return lazy_pinyin(res, 1)\n",
    "    else:\n",
    "        return lazy_pinyin(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    吼 吼 吼 ， 萌 死 人 的 棒 棒 糖 ， 中 了 大 众 点 评 的 霸 王 餐 ， ...\n",
       "1    第 三 次 参 加 大 众 点 评 网 霸 王 餐 的 活 动 。 这 家 店 给 人 整 ...\n",
       "2    4 人 同 行 点 了 1 0 个 小 吃 榴 莲 酥 榴 莲 味 道 不 足 松 软 奶 ...\n",
       "3    之 前 评 价 了 莫 名 其 妙 被 删 果 断 继 续 差 评 ！ 换 了 菜 单 价 ...\n",
       "4    出 乎 意 料 地 惊 艳 ， 椰 子 鸡 清 热 降 火 ， 美 容 养 颜 ， 大 大 ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(myconf.data.train_download)\n",
    "train.content = train.content.map(lambda x: filter_char(x))\n",
    "# train.content = train.content.map(lambda x: get_chars(x))\n",
    "# train.content = train.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "train[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(myconf.data.train, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    哎 ， 想 当 年 来 佘 山 的 时 候 ， 啥 都 没 有 ， 三 品 香 算 镇 上 ...\n",
       "1    趁 着 国 庆 节 ， 一 家 人 在 白 天 在 山 里 玩 耍 之 后 ， 晚 上 决 ...\n",
       "2    这 家 店 是 我 目 前 吃 到 的 最 干 净 的 串 串 店 ， 目 前 看 到 最 ...\n",
       "3    中 午 和 领 导 偷 偷 跑 出 来 开 小 灶 啦 ， 到 这 家 来 顿 料 理 ， ...\n",
       "4    拖 了 很 久 很 久 才 来 补 的 点 评 ， 之 前 和 朋 友 一 块 儿 去 的 ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv(myconf.data.val_download)\n",
    "val.content = val.content.map(lambda x: filter_char(x))\n",
    "# val.content = val.content.map(lambda x: get_chars(x))\n",
    "# val.content = val.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "val[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv(myconf.data.val, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    我 想 说 他 们 家 的 优 惠 活 动 好 持 久 啊 ， 我 预 售 的 时 候 买 ...\n",
       "1    终 于 开 到 心 心 念 念 的 L A B l o f t 。 第 一 次 来 就 随 ...\n",
       "2    地 理 位 置 好 ， 交 通 方 便 ， 就 在 1 2 4 车 站 对 面 交 通 方 ...\n",
       "3    运 气 很 好 ， 抽 中 了 大 众 点 评 的 霸 王 餐 。 这 家 主 题 餐 厅 ...\n",
       "4    幸 运 随 点 评 团 体 验 霸 王 餐 ， 心 情 好 ~ 蜀 九 香 刚 进 驻 泉 ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testa = pd.read_csv(myconf.data.testa_download)\n",
    "testa.content = testa.content.map(lambda x: filter_char(x))\n",
    "# testa.content = testa.content.map(lambda x: get_chars(x))\n",
    "# testa.content = testa.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "testa[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testa.to_csv(myconf.data.testa, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    可 以 说 工 作 日 中 午 的 这 个 套 餐 着 实 是 特 别 的 实 惠 啊 ， ...\n",
       "1    位 置 很 不 错 ， 老 板 眼 光 很 好 👍 位 于 地 铁 站 附 近 ， 周 围 ...\n",
       "2    和 朋 友 小 聚 ， 选 择 了 山 上 下 . 「 龙 虾 沙 拉 」 朋 友 不 喜 ...\n",
       "3    地 址 : 湖 滨 路 北 1 0 5 8 号 （ 国 税 局 南 侧 ） 东 来 顺 往 ...\n",
       "4    元 旦 中 午 来 吃 饭 ， 七 个 大 人 一 个 小 孩 。 第 一 次 吃 东 北 ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testb = pd.read_csv(myconf.data.testb_download)\n",
    "testb.content = testb.content.map(lambda x: filter_char(x))\n",
    "# testb.content = testb.content.map(lambda x: get_chars(x))\n",
    "# testb.content = testb.content.map(lambda x: get_pinyin2(x, tone=True))\n",
    "testb[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testb.to_csv(myconf.data.testb, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(srcfile=None, desfile=None, func=None, mode='w'):\n",
    "    \"\"\"按行读取源文件进行自定义处理，将结果保存到目标文件。\n",
    "    直到遇到空行或者到达文件末尾为止。\n",
    "    \"\"\"\n",
    "    assert srcfile is not None, \"srcfile can not be None\"\n",
    "    assert desfile is not None, \"desfile can not be None\"\n",
    "    assert func is not None, \"func can not be None\"\n",
    "    with open(desfile, mode, encoding='UTF-8') as des:\n",
    "        with open(srcfile, 'r', encoding='UTF-8') as src:\n",
    "            for line in iter(src.readline, ''):\n",
    "                des.write(func(line))\n",
    "    print(\"process_line complete!\")\n",
    "\n",
    "def build_seq(line):\n",
    "    \"\"\"Build seq for train data\"\"\"\n",
    "    items = line.rstrip().split(',')\n",
    "    tag = ' '.join(items[2:])\n",
    "    return items[1] + '\\t' + tag + '\\n'\n",
    "\n",
    "def build_seq_test(line):\n",
    "    \"\"\"Build seq for test data\"\"\"\n",
    "    items = line.rstrip().split(',')\n",
    "    return items[1] + '\\n'\n",
    "\n",
    "def build_text(line):\n",
    "    \"\"\"Build text for word2vec\"\"\"\n",
    "    items = line.rstrip().split('\\t')\n",
    "    return items[0] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_line complete!\n",
      "process_line complete!\n",
      "process_line complete!\n"
     ]
    }
   ],
   "source": [
    "# build data for seq2seq\n",
    "# 使用 filter_char 处理后去除表头生成 myconf.data\n",
    "process_line(srcfile=myconf.data.train, desfile=\"preprocess/train.tsv\", func=build_seq)\n",
    "process_line(srcfile=myconf.data.val, desfile=\"preprocess/val.tsv\", func=build_seq)\n",
    "process_line(srcfile=myconf.data.test, desfile=\"preprocess/test.tsv\", func=build_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_line complete!\n",
      "process_line complete!\n",
      "process_line complete!\n"
     ]
    }
   ],
   "source": [
    "# build data for word2vec\n",
    "# tsv 无表头\n",
    "process_line(srcfile=\"preprocess/base/train.tsv\", desfile=\"preprocess/base/train.txt\", func=build_text, mode='w')\n",
    "process_line(srcfile=\"preprocess/base/val.tsv\", desfile=\"preprocess/base/dev.txt\", func=build_text, mode='w')\n",
    "process_line(srcfile=\"preprocess/base/test.tsv\", desfile=\"preprocess/base/test.txt\", func=build_text, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_lines(filepath, start=0, nmax=0):\n",
    "    \"\"\"按行读取文件，读取指定行数 nmax，当 nmax=0 时按行读取全部\n",
    "    \"\"\"\n",
    "    fp = open(filepath, 'r', encoding='UTF-8')\n",
    "    if start > 0:\n",
    "        for i in range(start):\n",
    "            fp.readline()\n",
    "    n = 0\n",
    "    while True:\n",
    "        content = fp.readline()\n",
    "        if content == '' or (n >= nmax and nmax != 0):\n",
    "            break\n",
    "        n += 1\n",
    "        yield content      \n",
    "    fp.close()\n",
    "\n",
    "def cut_valid(filepath='./preprocessval.tsv', n=None, pos=None):\n",
    "    assert n is not None, \"n can not be None\"\n",
    "    assert pos is not None, \"pos can not be None\"\n",
    "    with open('./preprocess/val_{}.tsv'.format(str(pos)), 'w', encoding='UTF-8') as f:\n",
    "        for line in read_file_lines(filepath, start=0, nmax=pos):\n",
    "            f.write(line)\n",
    "    with open('./preprocess/val_{}.tsv'.format(str(n-pos)), 'w', encoding='UTF-8') as f:\n",
    "        for line in read_file_lines(filepath, start=pos, nmax=n-pos):\n",
    "            f.write(line)\n",
    "\n",
    "cut_valid(filepath='./preprocess/val.tsv', n=15000, pos=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# sentences = []\n",
    "# with open(\"./preprocess/chars.txt\", 'r', encoding='UTF-8') as src:\n",
    "#     for line in iter(src.readline, ''):\n",
    "#         sentences.append(line.rstrip().split())\n",
    "        \n",
    "frames = [train[\"content\"], val[\"content\"], testa[\"content\"], testb[\"content\"]]\n",
    "content = pd.concat(frames)\n",
    "with open(\"chars.txt\", 'w', encoding='UTF-8') as f:\n",
    "    for s in content:\n",
    "        f.write(s + '\\n')\n",
    "# sentences = [s for s in content]\n",
    "\n",
    "# word2vec_model = Word2Vec(\n",
    "#     sentences,\n",
    "#     size=100,\n",
    "#     window=5,\n",
    "#     min_count=1,\n",
    "#     workers=4,\n",
    "#     iter=15\n",
    "#     )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format(\"embedding/pinyin_all_wv.vector\", binary=True)\n",
    "word2vec_model.save(\"embedding/pinyin_all.vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format(\"embedding/chars_all_wv.vector\", binary=True)\n",
    "word2vec_model.save(\"embedding/chars_all.vector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
