{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed = 16\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ai_challenger_sentiment_analysis_trainingset_20180816/sentiment_analysis_trainingset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        stopwords.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segWord(doc):\n",
    "    seg_list = jieba.cut(doc, cut_all=False)\n",
    "    return list(seg_list)\n",
    "\n",
    "#move stop words\n",
    "def filter_map(arr):\n",
    "    res = \"\"\n",
    "    for c in arr:\n",
    "        if c not in stopwords and c != ' ' and c != '\\xa0'and c != '\\n' and c != '\\ufeff' and c != '\\r':\n",
    "            res += c\n",
    "    return res\n",
    "  \n",
    "#move stop words and generate char sent\n",
    "def filter_char_map(arr):\n",
    "    res = []\n",
    "    for c in arr:\n",
    "        if c not in stopwords and c != ' ' and c != '\\xa0'and c != '\\n' and c != '\\ufeff' and c != '\\r':\n",
    "            res.append(c)\n",
    "    return \" \".join(res)\n",
    "#get char of sentence\n",
    "def get_char(arr):\n",
    "    res = []\n",
    "    for c in arr:\n",
    "        res.append(c)\n",
    "    return list(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.content = data.content.map(lambda x: filter_map(x))\n",
    "data.content = data.content.map(lambda x: get_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [吼, 吼, 吼, 萌, 死, 人, 棒, 棒, 糖, 中, 大, 众, 点, 评, 霸, ...\n",
       "1    [三, 次, 参, 加, 大, 众, 点, 评, 网, 霸, 王, 餐, 活, 动, 家, ...\n",
       "2    [人, 行, 点, 小, 吃, 榴, 莲, 酥, 榴, 莲, 味, 道, 不, 足, 松, ...\n",
       "3    [前, 评, 价, 莫, 名, 妙, 删, 果, 断, 继, 续, 差, 评, 换, 菜, ...\n",
       "4    [出, 意, 料, 惊, 艳, 椰, 子, 鸡, 清, 热, 降, 火, 美, 容, 养, ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"preprocess/train_char.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_sent = []\n",
    "for s in data[\"content\"]:\n",
    "    line_sent.append(s)\n",
    "word2vec_model = Word2Vec(line_sent, size=100, window=10, min_count=1, workers=4, iter=15)\n",
    "word2vec_model.wv.save_word2vec_format(\"word2vec/chars.vector\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [想, 年, 佘, 山, 时, 候, 都, 没, 三, 品, 香, 算, 镇, 上, 最, ...\n",
       "1    [国, 庆, 节, 家, 人, 白, 天, 山, 里, 玩, 耍, 后, 晚, 上, 决, ...\n",
       "2    [家, 店, 目, 前, 吃, 最, 干, 净, 串, 串, 店, 目, 前, 看, 最, ...\n",
       "3    [中, 午, 领, 导, 偷, 偷, 跑, 出, 开, 小, 灶, 家, 顿, 料, 理, ...\n",
       "4    [拖, 很, 久, 很, 久, 才, 补, 点, 评, 前, 朋, 友, 块, 儿, 去, ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = pd.read_csv(\"ai_challenger_sentiment_analysis_validationset_20180816/sentiment_analysis_validationset.csv\")\n",
    "validation.content = validation.content.map(lambda x: filter_map(x))\n",
    "validation.content = validation.content.map(lambda x: get_char(x))\n",
    "validation[\"content\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.to_csv(\"preprocess/validation_char.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"ai_challenger_sentiment_analysis_testa_20180816/sentiment_analysis_testa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.content = test.content.map(lambda x: filter_map(x))\n",
    "test.content = test.content.map(lambda x: get_char(x))\n",
    "test.to_csv(\"preprocess/test_char.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
