# preprocess data parameters
max_len: 500
chinese_word_size: 195198 # add <num> to vocab
multi_categories: 20
num_sentiment: 4
vector_path: "./chinese_vectors/vectors.npy"

# dataset parameters
# batch_size: 1
batch_size: 32
prefetch: 2
buffer_size: 120000

# embed / position vectors parameters
embed_size: 300
hidden_size: 60 # reduce word dimension to 100

# AttnConv parameters

## transformer parameters
num_attention_stacks: 3
num_heads: 2

## Convolution parameters
filter_size_list:
  - 10
  # - 20
  # - 50
  # - 100
num_filters: 64  # num of feature maps generated by a filter
inner_dense_outshape:

# Optimization parameters
label_smooth: true
epsilon: 0.005
use_regularizer: true
reg_const: 0.001
max_norm: 5
dropout_rate: 0.1

# learning rate parameters
learning_rate: 0.01
momentum: 0.7
first_decay_steps: 3000
t_mul: 2.0  # t_mul times longer than previous time spent
m_mul: 0.9 # m_mul ** i * learning_rate when ith restart start
alpha: 0.0  # minimum lr during decay

# training parameters
train_steps: 45000
# train_steps: 92000
random_seed: 1024


# hook parameters
print_n_step: 20
save_n_step: 1000
keep_checkpoint_max: 20